{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, time, json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import json as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get 1,000 from Yelp! and some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using API to get the names, url and some features of the restaurants. Storing and running the function in data_1\n",
    "api_key ='iVEiMvbM9IdSVFlJ-icXHQrF1Yw7OMC2vAlZ_8yBwf_Fza4GQ7ey-RMN4OG7ADO0YvKJkVMDv1e3xzN_3g9mtfkDoXMSpWc3TdJBcJP3tJE-rMgzlG-pl4iTau1uXXYx'\n",
    "\n",
    "def all_restaurants(api_key, query, x,y):\n",
    "    headers = {'Authorization': 'Bearer %s'%api_key}\n",
    "    params = {'location': query,'categories':'restaurants'}\n",
    "    res = requests.get('https://api.yelp.com/v3/businesses/search', params = params, headers=headers).json()\n",
    "    aux1= []\n",
    "    for i in range(y,x,20):\n",
    "        time.sleep(2)\n",
    "        res2 = requests.get('https://api.yelp.com/v3/businesses/search', params = {'location': query, 'categories':'restaurants', 'offset':i}, headers=headers).json()\n",
    "        aux1.append(res2)\n",
    "    aux2=[]\n",
    "    for i in aux1:\n",
    "        for t in(i['businesses']):\n",
    "            aux2.append(t)\n",
    "    return aux2\n",
    "\n",
    "data_1 = all_restaurants(api_key,'Pittsburgh',1000,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping using BeautifulSoup, building in a function to handle the specifics of Yelp! webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function is build to extract the difference reviews for a given set of restaurants. The first part of the code gets the number of \n",
    "#reviews pages for a restaurants, using this as an input we scrap the 20% of the total number of pages.\n",
    "#We did this for two reason, to make it computationally feasible and because we are going to use only ratings between 2018-2019\n",
    "import math\n",
    "def extract_reviews(url):\n",
    "    alv = url\n",
    "    aux4 = requests.get(alv)\n",
    "    url2 = BeautifulSoup(aux4.content, \"html.parser\")\n",
    "    aux20=url2.find_all('div', attrs = {'class': \"lemon--div__373c0__1mboc u-padding-b2 border-color--default__373c0__2oFDT text-align--center__373c0__1l506\"})\n",
    "    aux21 = aux20[0].find('span',attrs = {'class':'lemon--span__373c0__3997G text__373c0__2pB8f text-color--inherit__373c0__w_15m text-align--left__373c0__2pnx_'}).text\n",
    "    aux22 = int(list(aux21.split(\" \"))[-1])\n",
    "    aux22 = math.ceil(aux22*0.2) #20% of the total number of pages\n",
    "    list3 = [i*20 for i in range(0,aux22)]\n",
    "    list5 = [alv+\"&start=\"+str(i) for i in list3]\n",
    "    lst3 =[]\n",
    "    for i in list5:\n",
    "        aux11 = requests.get(i)\n",
    "        aux6 = BeautifulSoup(aux11.content, \"html.parser\")\n",
    "        aux7 = aux6.find_all('div',attrs={'itemprop':'review'})\n",
    "        for i in aux7:\n",
    "            aut = i.find('meta',attrs={'itemprop':'author'} ).get('content') #get author name\n",
    "            date = i.find('meta',attrs={'itemprop':'datePublished'} ).get('content') #get date publish name\n",
    "            des = i.find('p',attrs={'itemprop':'description'} ).text #get review\n",
    "            rat = float(i.find('meta',attrs={'itemprop':'ratingValue'} ).get('content')) #get rating\n",
    "            di = {'author':aut,'rating':rat,'date':date,'description':des}\n",
    "            lst3.append(di) \n",
    "    return lst3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the functions and saving in Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This makes scraping easier and we can parallelize\n",
    "reviews_dic_1 = {}\n",
    "for i in data_1[0:50]:\n",
    "    get_id = i['id']\n",
    "    get_url = i['url']\n",
    "    reviews = extract_reviews(url=get_url)\n",
    "    reviews_dic_1[get_id] = reviews\n",
    "\n",
    "reviews_dic_2 = {}\n",
    "for i in data_1[50:250]:\n",
    "    get_id = i['id']\n",
    "    get_url = i['url']\n",
    "    reviews = extract_reviews(url=get_url)\n",
    "    reviews_dic_2[get_id] = reviews \n",
    "\n",
    "reviews_dic_3 = {}\n",
    "for i in data_1[250:500]:\n",
    "    get_id = i['id']\n",
    "    get_url = i['url']\n",
    "    reviews = extract_reviews(url=get_url)\n",
    "    reviews_dic_3[get_id] = reviews   \n",
    "    \n",
    "reviews_dic_4 = {}\n",
    "for i in data_1[500:750]:\n",
    "    get_id = i['id']\n",
    "    get_url = i['url']\n",
    "    reviews = extract_reviews(url=get_url)\n",
    "    reviews_dic_4[get_id] = reviews   \n",
    "    \n",
    "reviews_dic_5 = {}\n",
    "for i in data_1[750:1000]:\n",
    "    get_id = i['id']\n",
    "    get_url = i['url']\n",
    "    reviews = extract_reviews(url=get_url)\n",
    "    reviews_dic_5[get_id] = reviews  \n",
    "    \n",
    "json1 = JSON.dumps(reviews_dic_1)\n",
    "f = open(\"reviews_0_50.json\",\"w\")\n",
    "f.write(json1)\n",
    "f.close()\n",
    "\n",
    "json2 = JSON.dumps(reviews_dic_2)\n",
    "f = open(\"reviews_50_250.json\",\"w\")\n",
    "f.write(json2)\n",
    "f.close()\n",
    "\n",
    "json3 = JSON.dumps(reviews_dic_3)\n",
    "f = open(\"reviews_250_500.json\",\"w\")\n",
    "f.write(json3)\n",
    "f.close()\n",
    "\n",
    "json4 = JSON.dumps(reviews_dic_4)\n",
    "f = open(\"reviews_500_750.json\",\"w\")\n",
    "f.write(json4)\n",
    "f.close()\n",
    "\n",
    "json5 = JSON.dumps(reviews_dic_5)\n",
    "f = open(\"reviews_750_1000.json\",\"w\")\n",
    "f.write(json5)\n",
    "f.close()\n",
    "\n",
    "json = JSON.dumps(data_1)\n",
    "f = open(\"data_1.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
